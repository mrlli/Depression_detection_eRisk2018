{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%time\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import sklearn\n",
    "\n",
    "# Libraries and packages for text (pre-)processing \n",
    "import string\n",
    "import re\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>DATE</th>\n",
       "      <th>INFO</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-18 23:49:34</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>\"Kill Bill Siren\" was awesome. These seriously...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-17 21:22:47</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>crazy bastard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-17 17:30:23</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>What I hoped for this post: something other th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-16 21:55:42</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>I didn't find this scene funny, but the fact t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-16 21:36:35</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>Bill from Kill Bill. Not saying the five point...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID TITLE                 DATE         INFO  \\\n",
       "0  test_subject1005   NaN  2015-03-18 23:49:34  reddit post   \n",
       "1  test_subject1005   NaN  2015-03-17 21:22:47  reddit post   \n",
       "2  test_subject1005   NaN  2015-03-17 17:30:23  reddit post   \n",
       "3  test_subject1005   NaN  2015-03-16 21:55:42  reddit post   \n",
       "4  test_subject1005   NaN  2015-03-16 21:36:35  reddit post   \n",
       "\n",
       "                                                TEXT  Depression  \n",
       "0  \"Kill Bill Siren\" was awesome. These seriously...           0  \n",
       "1                                      crazy bastard           0  \n",
       "2  What I hoped for this post: something other th...           0  \n",
       "3  I didn't find this scene funny, but the fact t...           0  \n",
       "4  Bill from Kill Bill. Not saying the five point...           0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n",
    "#test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 531394 entries, 0 to 531393\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   ID          531394 non-null  object\n",
      " 1   TITLE       163955 non-null  object\n",
      " 2   DATE        531394 non-null  object\n",
      " 3   INFO        531394 non-null  object\n",
      " 4   TEXT        389544 non-null  object\n",
      " 5   Depression  531394 non-null  int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 24.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 544447 entries, 0 to 544446\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   ID          544447 non-null  object\n",
      " 1   TITLE       177602 non-null  object\n",
      " 2   DATE        544447 non-null  object\n",
      " 3   INFO        544447 non-null  object\n",
      " 4   TEXT        397250 non-null  object\n",
      " 5   Depression  544447 non-null  int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 24.9+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TEXT'] = train['TITLE'].fillna('-') + \" \" + train['TEXT'].fillna('-')\n",
    "test['TEXT'] = test['TITLE'].fillna('-') + \" \" + test['TEXT'].fillna('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['DATE','INFO','TITLE'],inplace= True,axis=1)\n",
    "test.drop(['DATE','INFO','TITLE'],inplace= True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 531394 entries, 0 to 531393\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   ID          531394 non-null  object\n",
      " 1   TEXT        531394 non-null  object\n",
      " 2   Depression  531394 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 12.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 544447 entries, 0 to 544446\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   ID          544447 non-null  object\n",
      " 1   TEXT        544447 non-null  object\n",
      " 2   Depression  544447 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 12.5+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>- \"Kill Bill Siren\" was awesome. These serious...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>- crazy bastard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>- What I hoped for this post: something other ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>- I didn't find this scene funny, but the fact...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>- Bill from Kill Bill. Not saying the five poi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                               TEXT  \\\n",
       "0  test_subject1005  - \"Kill Bill Siren\" was awesome. These serious...   \n",
       "1  test_subject1005                                    - crazy bastard   \n",
       "2  test_subject1005  - What I hoped for this post: something other ...   \n",
       "3  test_subject1005  - I didn't find this scene funny, but the fact...   \n",
       "4  test_subject1005  - Bill from Kill Bill. Not saying the five poi...   \n",
       "\n",
       "   Depression  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  text preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "#set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lower_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"- Bill from Kill Bill. Not saying the five point palm exploding heart technique sucks....but he's a dude we have waited through TWO movies to see die. And he falls. On the grass. Cool.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TEXT'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(df):\n",
    "    df['TEXT'] = df['TEXT'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    print(df['TEXT'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    - \"kill bill siren\" was awesome. these serious...\n",
      "1                                      - crazy bastard\n",
      "2    - what i hoped for this post: something other ...\n",
      "3    - i didn't find this scene funny, but the fact...\n",
      "4    - bill from kill bill. not saying the five poi...\n",
      "Name: TEXT, dtype: object\n",
      "0    til that saigas, a species of antelope with un...\n",
      "1    what did bernie sanders say when he found a de...\n",
      "2    - i didn't have any trouble with faux iosefka ...\n",
      "3    - did you have trouble with yurie and faux ios...\n",
      "4    - ;) what about the bloody crow of cainhurst? ...\n",
      "Name: TEXT, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lower_case(train)\n",
    "lower_case(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"- i didn't find this scene funny, but the fact that you did kills me.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TEXT'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand the Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "def expand_contractions(df):\n",
    "    df['TEXT'] = df['TEXT'].apply(lambda x: contractions.fix(x))\n",
    "    print(df['TEXT'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"- i didn't find this scene funny, but the fact that you did kills me.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TEXT'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    - \"kill bill siren\" was awesome. these serious...\n",
      "1                                      - crazy bastard\n",
      "2    - what i hoped for this post: something other ...\n",
      "3    - i did not find this scene funny, but the fac...\n",
      "4    - bill from kill bill. not saying the five poi...\n",
      "Name: TEXT, dtype: object\n",
      "0    til that saigas, a species of antelope with un...\n",
      "1    what did bernie sanders say when he found a de...\n",
      "2    - i did not have any trouble with faux iosefka...\n",
      "3    - did you have trouble with yurie and faux ios...\n",
      "4    - ;) what about the bloody crow of cainhurst? ...\n",
      "Name: TEXT, dtype: object\n"
     ]
    }
   ],
   "source": [
    "expand_contractions(train)\n",
    "expand_contractions(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- i did not find this scene funny, but the fact that you did kills me.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TEXT'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noisy removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- its the same format used in https://www.reddit.com/r/redditwritesseinfeld'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TEXT'][278]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "\n",
    "\n",
    "def url_remove(df):\n",
    "    df['TEXT'] = df['TEXT'].apply(lambda x: remove_URL(x))\n",
    "    print(df['TEXT'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    - \"kill bill siren\" was awesome. these serious...\n",
      "1                                      - crazy bastard\n",
      "2    - what i hoped for this post: something other ...\n",
      "3    - i did not find this scene funny, but the fac...\n",
      "4    - bill from kill bill. not saying the five poi...\n",
      "Name: TEXT, dtype: object\n",
      "0    til that saigas, a species of antelope with un...\n",
      "1    what did bernie sanders say when he found a de...\n",
      "2    - i did not have any trouble with faux iosefka...\n",
      "3    - did you have trouble with yurie and faux ios...\n",
      "4    - ;) what about the bloody crow of cainhurst? ...\n",
      "Name: TEXT, dtype: object\n"
     ]
    }
   ],
   "source": [
    "url_remove(train)\n",
    "url_remove(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- its the same format used in '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TEXT'][278]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML tags remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- its the same format used in '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TEXT'][278]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "    return re.sub(html, \"\", text)\n",
    "\n",
    "def html_remove(df):\n",
    "    df['TEXT'] = df['TEXT'].apply(lambda x: remove_html(x))\n",
    "    print(df['TEXT'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    - \"kill bill siren\" was awesome. these serious...\n",
      "1                                      - crazy bastard\n",
      "2    - what i hoped for this post: something other ...\n",
      "3    - i did not find this scene funny, but the fac...\n",
      "4    - bill from kill bill. not saying the five poi...\n",
      "Name: TEXT, dtype: object\n",
      "0    til that saigas, a species of antelope with un...\n",
      "1    what did bernie sanders say when he found a de...\n",
      "2    - i did not have any trouble with faux iosefka...\n",
      "3    - did you have trouble with yurie and faux ios...\n",
      "4    - ;) what about the bloody crow of cainhurst? ...\n",
      "Name: TEXT, dtype: object\n"
     ]
    }
   ],
   "source": [
    "html_remove(train)\n",
    "html_remove(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## punctuation_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_removal(df):\n",
    "    df['TEXT'] = df['TEXT'].str.replace('[^\\w\\s]','')\n",
    "    print(df['TEXT'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- \"kill bill siren\" was awesome. these seriously make my week. you should do them for better call saul!!! that subreddit would be extremely grateful.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TEXT'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     kill bill siren was awesome these seriously m...\n",
      "1                                        crazy bastard\n",
      "2     what i hoped for this post something other th...\n",
      "3     i did not find this scene funny but the fact ...\n",
      "4     bill from kill bill not saying the five point...\n",
      "Name: TEXT, dtype: object\n",
      "0    til that saigas a species of antelope with unu...\n",
      "1    what did bernie sanders say when he found a de...\n",
      "2     i did not have any trouble with faux iosefka ...\n",
      "3     did you have trouble with yurie and faux iose...\n",
      "4      what about the bloody crow of cainhurst not ...\n",
      "Name: TEXT, dtype: object\n"
     ]
    }
   ],
   "source": [
    "punctuation_removal(train)\n",
    "punctuation_removal(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' kill bill siren was awesome these seriously make my week you should do them for better call saul that subreddit would be extremely grateful'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TEXT'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace typos,slang,acronyms or informal abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' mahouka sure tries hard to make the antidiscrimination course 2 students look like extremists fuck them not being happy with discrimination amirite there is a difference between institutionalized discrimination and personal bigotry they seemed to be fighting against the latter when the former is far more important the former could be changed with progressive policies while the latter cannot obviously bigotry is not ok but like mayumi said you cannot change what an individual feels by force'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TEXT'][2154]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_clean(text):\n",
    "        # Typos, slang and other\n",
    "        sample_typos_slang = {\n",
    "                                \"w/e\": \"whatever\",\n",
    "                                \"usagov\": \"usa government\",\n",
    "                                \"recentlu\": \"recently\",\n",
    "                                \"ph0tos\": \"photos\",\n",
    "                                \"amirite\": \"am i right\",\n",
    "                                \"exp0sed\": \"exposed\",\n",
    "                                \"<3\": \"love\",\n",
    "                                \"luv\": \"love\",\n",
    "                                \"amageddon\": \"armageddon\",\n",
    "                                \"trfc\": \"traffic\",\n",
    "                                \"16yr\": \"16 year\"\n",
    "                                }\n",
    "\n",
    "        # Acronyms\n",
    "        sample_acronyms =  { \n",
    "                            \"mh370\": \"malaysia airlines flight 370\",\n",
    "                            \"okwx\": \"oklahoma city weather\",\n",
    "                            \"arwx\": \"arkansas weather\",    \n",
    "                            \"gawx\": \"georgia weather\",  \n",
    "                            \"scwx\": \"south carolina weather\",  \n",
    "                            \"cawx\": \"california weather\",\n",
    "                            \"tnwx\": \"tennessee weather\",\n",
    "                            \"azwx\": \"arizona weather\",  \n",
    "                            \"alwx\": \"alabama weather\",\n",
    "                            \"usnwsgov\": \"united states national weather service\",\n",
    "                            \"2mw\": \"tomorrow\"\n",
    "                            }\n",
    "\n",
    "        \n",
    "        # Some common abbreviations \n",
    "        sample_abbr = {\n",
    "                        \"$\" : \" dollar \",\n",
    "                        \"â‚¬\" : \" euro \",\n",
    "                        \"4ao\" : \"for adults only\",\n",
    "                        \"a.m\" : \"before midday\",\n",
    "                        \"a3\" : \"anytime anywhere anyplace\",\n",
    "                        \"aamof\" : \"as a matter of fact\",\n",
    "                        \"acct\" : \"account\",\n",
    "                        \"adih\" : \"another day in hell\",\n",
    "                        \"afaic\" : \"as far as i am concerned\",\n",
    "                        \"afaict\" : \"as far as i can tell\",\n",
    "                        \"afaik\" : \"as far as i know\",\n",
    "                        \"afair\" : \"as far as i remember\",\n",
    "                        \"afk\" : \"away from keyboard\",\n",
    "                        \"app\" : \"application\",\n",
    "                        \"approx\" : \"approximately\",\n",
    "                        \"apps\" : \"applications\",\n",
    "                        \"asap\" : \"as soon as possible\",\n",
    "                        \"asl\" : \"age, sex, location\",\n",
    "                        \"atk\" : \"at the keyboard\",\n",
    "                        \"ave.\" : \"avenue\",\n",
    "                        \"aymm\" : \"are you my mother\",\n",
    "                        \"ayor\" : \"at your own risk\", \n",
    "                        \"b&b\" : \"bed and breakfast\",\n",
    "                        \"b+b\" : \"bed and breakfast\",\n",
    "                        \"b.c\" : \"before christ\",\n",
    "                        \"b2b\" : \"business to business\",\n",
    "                        \"b2c\" : \"business to customer\",\n",
    "                        \"b4\" : \"before\",\n",
    "                        \"b4n\" : \"bye for now\",\n",
    "                        \"b@u\" : \"back at you\",\n",
    "                        \"bae\" : \"before anyone else\",\n",
    "                        \"bak\" : \"back at keyboard\",\n",
    "                        \"bbbg\" : \"bye bye be good\",\n",
    "                        \"bbc\" : \"british broadcasting corporation\",\n",
    "                        \"bbias\" : \"be back in a second\",\n",
    "                        \"bbl\" : \"be back later\",\n",
    "                        \"bbs\" : \"be back soon\",\n",
    "                        \"be4\" : \"before\",\n",
    "                        \"bfn\" : \"bye for now\",\n",
    "                        \"blvd\" : \"boulevard\",\n",
    "                        \"bout\" : \"about\",\n",
    "                        \"brb\" : \"be right back\",\n",
    "                        \"bros\" : \"brothers\",\n",
    "                        \"brt\" : \"be right there\",\n",
    "                        \"bsaaw\" : \"big smile and a wink\",\n",
    "                        \"btw\" : \"by the way\",\n",
    "                        \"bwl\" : \"bursting with laughter\",\n",
    "                        \"c/o\" : \"care of\",\n",
    "                        \"cet\" : \"central european time\",\n",
    "                        \"cf\" : \"compare\",\n",
    "                        \"cia\" : \"central intelligence agency\",\n",
    "                        \"csl\" : \"can not stop laughing\",\n",
    "                        \"cu\" : \"see you\",\n",
    "                        \"cul8r\" : \"see you later\",\n",
    "                        \"cv\" : \"curriculum vitae\",\n",
    "                        \"cwot\" : \"complete waste of time\",\n",
    "                        \"cya\" : \"see you\",\n",
    "                        \"cyt\" : \"see you tomorrow\",\n",
    "                        \"dae\" : \"does anyone else\",\n",
    "                        \"dbmib\" : \"do not bother me i am busy\",\n",
    "                        \"diy\" : \"do it yourself\",\n",
    "                        \"dm\" : \"direct message\",\n",
    "                        \"dwh\" : \"during work hours\",\n",
    "                        \"e123\" : \"easy as one two three\",\n",
    "                        \"eet\" : \"eastern european time\",\n",
    "                        \"eg\" : \"example\",\n",
    "                        \"embm\" : \"early morning business meeting\",\n",
    "                        \"encl\" : \"enclosed\",\n",
    "                        \"encl.\" : \"enclosed\",\n",
    "                        \"etc\" : \"and so on\",\n",
    "                        \"faq\" : \"frequently asked questions\",\n",
    "                        \"fawc\" : \"for anyone who cares\",\n",
    "                        \"fb\" : \"facebook\",\n",
    "                        \"fc\" : \"fingers crossed\",\n",
    "                        \"fig\" : \"figure\",\n",
    "                        \"fimh\" : \"forever in my heart\", \n",
    "                        \"ft.\" : \"feet\",\n",
    "                        \"ft\" : \"featuring\",\n",
    "                        \"ftl\" : \"for the loss\",\n",
    "                        \"ftw\" : \"for the win\",\n",
    "                        \"fwiw\" : \"for what it is worth\",\n",
    "                        \"fyi\" : \"for your information\",\n",
    "                        \"g9\" : \"genius\",\n",
    "                        \"gahoy\" : \"get a hold of yourself\",\n",
    "                        \"gal\" : \"get a life\",\n",
    "                        \"gcse\" : \"general certificate of secondary education\",\n",
    "                        \"gfn\" : \"gone for now\",\n",
    "                        \"gg\" : \"good game\",\n",
    "                        \"gl\" : \"good luck\",\n",
    "                        \"glhf\" : \"good luck have fun\",\n",
    "                        \"gmt\" : \"greenwich mean time\",\n",
    "                        \"gmta\" : \"great minds think alike\",\n",
    "                        \"gn\" : \"good night\",\n",
    "                        \"g.o.a.t\" : \"greatest of all time\",\n",
    "                        \"goat\" : \"greatest of all time\",\n",
    "                        \"goi\" : \"get over it\",\n",
    "                        \"gps\" : \"global positioning system\",\n",
    "                        \"gr8\" : \"great\",\n",
    "                        \"gratz\" : \"congratulations\",\n",
    "                        \"gyal\" : \"girl\",\n",
    "                        \"h&c\" : \"hot and cold\",\n",
    "                        \"hp\" : \"horsepower\",\n",
    "                        \"hr\" : \"hour\",\n",
    "                        \"hrh\" : \"his royal highness\",\n",
    "                        \"ht\" : \"height\",\n",
    "                        \"ibrb\" : \"i will be right back\",\n",
    "                        \"ic\" : \"i see\",\n",
    "                        \"icq\" : \"i seek you\",\n",
    "                        \"icymi\" : \"in case you missed it\",\n",
    "                        \"idc\" : \"i do not care\",\n",
    "                        \"idgadf\" : \"i do not give a damn fuck\",\n",
    "                        \"idgaf\" : \"i do not give a fuck\",\n",
    "                        \"idk\" : \"i do not know\",\n",
    "                        \"ie\" : \"that is\",\n",
    "                        \"i.e\" : \"that is\",\n",
    "                        \"ifyp\" : \"i feel your pain\",\n",
    "                        \"IG\" : \"instagram\",\n",
    "                        \"iirc\" : \"if i remember correctly\",\n",
    "                        \"ilu\" : \"i love you\",\n",
    "                        \"ily\" : \"i love you\",\n",
    "                        \"imho\" : \"in my humble opinion\",\n",
    "                        \"imo\" : \"in my opinion\",\n",
    "                        \"imu\" : \"i miss you\",\n",
    "                        \"iow\" : \"in other words\",\n",
    "                        \"irl\" : \"in real life\",\n",
    "                        \"j4f\" : \"just for fun\",\n",
    "                        \"jic\" : \"just in case\",\n",
    "                        \"jk\" : \"just kidding\",\n",
    "                        \"jsyk\" : \"just so you know\",\n",
    "                        \"l8r\" : \"later\",\n",
    "                        \"lb\" : \"pound\",\n",
    "                        \"lbs\" : \"pounds\",\n",
    "                        \"ldr\" : \"long distance relationship\",\n",
    "                        \"lmao\" : \"laugh my ass off\",\n",
    "                        \"lmfao\" : \"laugh my fucking ass off\",\n",
    "                        \"lol\" : \"laughing out loud\",\n",
    "                        \"ltd\" : \"limited\",\n",
    "                        \"ltns\" : \"long time no see\",\n",
    "                        \"m8\" : \"mate\",\n",
    "                        \"mf\" : \"motherfucker\",\n",
    "                        \"mfs\" : \"motherfuckers\",\n",
    "                        \"mfw\" : \"my face when\",\n",
    "                        \"mofo\" : \"motherfucker\",\n",
    "                        \"mph\" : \"miles per hour\",\n",
    "                        \"mr\" : \"mister\",\n",
    "                        \"mrw\" : \"my reaction when\",\n",
    "                        \"ms\" : \"miss\",\n",
    "                        \"mte\" : \"my thoughts exactly\",\n",
    "                        \"nagi\" : \"not a good idea\",\n",
    "                        \"nbc\" : \"national broadcasting company\",\n",
    "                        \"nbd\" : \"not big deal\",\n",
    "                        \"nfs\" : \"not for sale\",\n",
    "                        \"ngl\" : \"not going to lie\",\n",
    "                        \"nhs\" : \"national health service\",\n",
    "                        \"nrn\" : \"no reply necessary\",\n",
    "                        \"nsfl\" : \"not safe for life\",\n",
    "                        \"nsfw\" : \"not safe for work\",\n",
    "                        \"nth\" : \"nice to have\",\n",
    "                        \"nvr\" : \"never\",\n",
    "                        \"nyc\" : \"new york city\",\n",
    "                        \"oc\" : \"original content\",\n",
    "                        \"og\" : \"original\",\n",
    "                        \"ohp\" : \"overhead projector\",\n",
    "                        \"oic\" : \"oh i see\",\n",
    "                        \"omdb\" : \"over my dead body\",\n",
    "                        \"omg\" : \"oh my god\",\n",
    "                        \"omw\" : \"on my way\",\n",
    "                        \"p.a\" : \"per annum\",\n",
    "                        \"p.m\" : \"after midday\",\n",
    "                        \"pm\" : \"prime minister\",\n",
    "                        \"poc\" : \"people of color\",\n",
    "                        \"pov\" : \"point of view\",\n",
    "                        \"pp\" : \"pages\",\n",
    "                        \"ppl\" : \"people\",\n",
    "                        \"prw\" : \"parents are watching\",\n",
    "                        \"ps\" : \"postscript\",\n",
    "                        \"pt\" : \"point\",\n",
    "                        \"ptb\" : \"please text back\",\n",
    "                        \"pto\" : \"please turn over\",\n",
    "                        \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "                        \"ratchet\" : \"rude\",\n",
    "                        \"rbtl\" : \"read between the lines\",\n",
    "                        \"rlrt\" : \"real life retweet\", \n",
    "                        \"rofl\" : \"rolling on the floor laughing\",\n",
    "                        \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "                        \"rt\" : \"retweet\",\n",
    "                        \"ruok\" : \"are you ok\",\n",
    "                        \"sfw\" : \"safe for work\",\n",
    "                        \"sk8\" : \"skate\",\n",
    "                        \"smh\" : \"shake my head\",\n",
    "                        \"sq\" : \"square\",\n",
    "                        \"srsly\" : \"seriously\", \n",
    "                        \"ssdd\" : \"same stuff different day\",\n",
    "                        \"tbh\" : \"to be honest\",\n",
    "                        \"tbs\" : \"tablespooful\",\n",
    "                        \"tbsp\" : \"tablespooful\",\n",
    "                        \"tfw\" : \"that feeling when\",\n",
    "                        \"thks\" : \"thank you\",\n",
    "                        \"tho\" : \"though\",\n",
    "                        \"thx\" : \"thank you\",\n",
    "                        \"tia\" : \"thanks in advance\",\n",
    "                        \"til\" : \"today i learned\",\n",
    "                        \"tl;dr\" : \"too long i did not read\",\n",
    "                        \"tldr\" : \"too long i did not read\",\n",
    "                        \"tmb\" : \"tweet me back\",\n",
    "                        \"tntl\" : \"trying not to laugh\",\n",
    "                        \"ttyl\" : \"talk to you later\",\n",
    "                        \"u\" : \"you\",\n",
    "                        \"u2\" : \"you too\",\n",
    "                        \"u4e\" : \"yours for ever\",\n",
    "                        \"utc\" : \"coordinated universal time\",\n",
    "                        \"w/\" : \"with\",\n",
    "                        \"w/o\" : \"without\",\n",
    "                        \"w8\" : \"wait\",\n",
    "                        \"wassup\" : \"what is up\",\n",
    "                        \"wb\" : \"welcome back\",\n",
    "                        \"wtf\" : \"what the fuck\",\n",
    "                        \"wtg\" : \"way to go\",\n",
    "                        \"wtpa\" : \"where the party at\",\n",
    "                        \"wuf\" : \"where are you from\",\n",
    "                        \"wuzup\" : \"what is up\",\n",
    "                        \"wywh\" : \"wish you were here\",\n",
    "                        \"yd\" : \"yard\",\n",
    "                        \"ygtr\" : \"you got that right\",\n",
    "                        \"ynk\" : \"you never know\",\n",
    "                        \"zzz\" : \"sleeping bored and tired\"\n",
    "                        }\n",
    "            \n",
    "        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n",
    "        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n",
    "        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n",
    "        \n",
    "        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n",
    "        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n",
    "        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_other(df):\n",
    "    df['TEXT'] = df['TEXT'].apply(lambda x: other_clean(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_other(train)\n",
    "replace_other(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' mahouka sure tries hard to make the antidiscrimination course 2 students look like extremists fuck them not being happy with discrimination am i right there is a difference between institutionalized discrimination and personal bigotry they seemed to be fighting against the latter when the former is far more important the former could be changed with progressive policies while the latter cannot obviously bigotry is not ok but like mayumi said you cannot change what an individual feels by force'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TEXT'][2154]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>kill bill siren was awesome these seriously m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>crazy bastard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>what i hoped for this post something other th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>i did not find this scene funny but the fact ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>bill from kill bill not saying the five point...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                               TEXT  \\\n",
       "0  test_subject1005   kill bill siren was awesome these seriously m...   \n",
       "1  test_subject1005                                      crazy bastard   \n",
       "2  test_subject1005   what i hoped for this post something other th...   \n",
       "3  test_subject1005   i did not find this scene funny but the fact ...   \n",
       "4  test_subject1005   bill from kill bill not saying the five point...   \n",
       "\n",
       "   Depression  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop_words_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_removal(df):\n",
    "    df['TEXT'] = df['TEXT'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    print(df['TEXT'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    kill bill siren awesome seriously make week be...\n",
      "1                                        crazy bastard\n",
      "2                         hoped post something obvious\n",
      "3                          find scene funny fact kills\n",
      "4    bill kill bill saying five point palm explodin...\n",
      "Name: TEXT, dtype: object\n",
      "0    today learned saigas species antelope unusuall...\n",
      "1    bernie sanders say found dead body democratic ...\n",
      "2    trouble faux iosefka yurie pretty difficult st...\n",
      "3                      trouble yurie faux iosefka well\n",
      "4      bloody crow cainhurst sure counts boss fog door\n",
      "Name: TEXT, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words_removal(train)\n",
    "stop_words_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spell_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction(df):\n",
    "    return df['TEXT'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    kill bill siren awesome seriously make week be...\n",
      "1                                        crazy bastard\n",
      "2                         hoped post something obvious\n",
      "3                          find scene funny fact kills\n",
      "4    bill kill bill saying five point palm explodin...\n",
      "Name: TEXT, dtype: object\n",
      "0    today learned saigas species antelope unusuall...\n",
      "1    bernie sanders say found dead body democratic ...\n",
      "2    trouble faux iosefka yurie pretty difficult st...\n",
      "3                      trouble yurie faux iosefka well\n",
      "4      bloody crow cainhurst sure counts boss fog door\n",
      "Name: TEXT, dtype: object\n"
     ]
    }
   ],
   "source": [
    "spell_correction(train)\n",
    "spell_correction(test)\n",
    "print(train['TEXT'].head())\n",
    "print(test['TEXT'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the tweet base texts.\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(df):\n",
    "    df['tokenized'] = df['TEXT'].apply(word_tokenize)\n",
    "    print(df['TEXT'].head())\n",
    "    print(df['tokenized'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    kill bill siren awesome seriously make week be...\n",
      "1                                        crazy bastard\n",
      "2                         hoped post something obvious\n",
      "3                          find scene funny fact kills\n",
      "4    bill kill bill saying five point palm explodin...\n",
      "Name: TEXT, dtype: object\n",
      "0    [kill, bill, siren, awesome, seriously, make, ...\n",
      "1                                     [crazy, bastard]\n",
      "2                    [hoped, post, something, obvious]\n",
      "3                    [find, scene, funny, fact, kills]\n",
      "4    [bill, kill, bill, saying, five, point, palm, ...\n",
      "Name: tokenized, dtype: object\n",
      "0    today learned saigas species antelope unusuall...\n",
      "1    bernie sanders say found dead body democratic ...\n",
      "2    trouble faux iosefka yurie pretty difficult st...\n",
      "3                      trouble yurie faux iosefka well\n",
      "4      bloody crow cainhurst sure counts boss fog door\n",
      "Name: TEXT, dtype: object\n",
      "0    [today, learned, saigas, species, antelope, un...\n",
      "1    [bernie, sanders, say, found, dead, body, demo...\n",
      "2    [trouble, faux, iosefka, yurie, pretty, diffic...\n",
      "3                [trouble, yurie, faux, iosefka, well]\n",
      "4    [bloody, crow, cainhurst, sure, counts, boss, ...\n",
      "Name: tokenized, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tokens(train)\n",
    "tokens(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def tokens(df):\\n    return TextBlob(df['TEXT'][1]).words\\n\\ntokens(train)\\ntokens(test)\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def tokens(df):\n",
    "    return TextBlob(df['TEXT'][1]).words\n",
    "\n",
    "tokens(train)\n",
    "tokens(test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Depression</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>kill bill siren awesome seriously make week be...</td>\n",
       "      <td>0</td>\n",
       "      <td>[kill, bill, siren, awesome, seriously, make, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>crazy bastard</td>\n",
       "      <td>0</td>\n",
       "      <td>[crazy, bastard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>hoped post something obvious</td>\n",
       "      <td>0</td>\n",
       "      <td>[hoped, post, something, obvious]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>find scene funny fact kills</td>\n",
       "      <td>0</td>\n",
       "      <td>[find, scene, funny, fact, kills]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>bill kill bill saying five point palm explodin...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bill, kill, bill, saying, five, point, palm, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                               TEXT  \\\n",
       "0  test_subject1005  kill bill siren awesome seriously make week be...   \n",
       "1  test_subject1005                                      crazy bastard   \n",
       "2  test_subject1005                       hoped post something obvious   \n",
       "3  test_subject1005                        find scene funny fact kills   \n",
       "4  test_subject1005  bill kill bill saying five point palm explodin...   \n",
       "\n",
       "   Depression                                          tokenized  \n",
       "0           0  [kill, bill, siren, awesome, seriously, make, ...  \n",
       "1           0                                   [crazy, bastard]  \n",
       "2           0                  [hoped, post, something, obvious]  \n",
       "3           0                  [find, scene, funny, fact, kills]  \n",
       "4           0  [bill, kill, bill, saying, five, point, palm, ...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def porter_stemmer(text):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    stems = [stemmer.stem(i) for i in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(df):\n",
    "    df['porter_stmmer'] = df['tokenized'].apply(lambda x: porter_stemmer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "stemming(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "stemming(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Depression</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>porter_stmmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>kill bill siren awesome seriously make week be...</td>\n",
       "      <td>0</td>\n",
       "      <td>[kill, bill, siren, awesome, seriously, make, ...</td>\n",
       "      <td>[kill, bill, siren, awesom, serious, make, wee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>crazy bastard</td>\n",
       "      <td>0</td>\n",
       "      <td>[crazy, bastard]</td>\n",
       "      <td>[crazi, bastard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>hoped post something obvious</td>\n",
       "      <td>0</td>\n",
       "      <td>[hoped, post, something, obvious]</td>\n",
       "      <td>[hope, post, someth, obviou]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>find scene funny fact kills</td>\n",
       "      <td>0</td>\n",
       "      <td>[find, scene, funny, fact, kills]</td>\n",
       "      <td>[find, scene, funni, fact, kill]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>bill kill bill saying five point palm explodin...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bill, kill, bill, saying, five, point, palm, ...</td>\n",
       "      <td>[bill, kill, bill, say, five, point, palm, exp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                               TEXT  \\\n",
       "0  test_subject1005  kill bill siren awesome seriously make week be...   \n",
       "1  test_subject1005                                      crazy bastard   \n",
       "2  test_subject1005                       hoped post something obvious   \n",
       "3  test_subject1005                        find scene funny fact kills   \n",
       "4  test_subject1005  bill kill bill saying five point palm explodin...   \n",
       "\n",
       "   Depression                                          tokenized  \\\n",
       "0           0  [kill, bill, siren, awesome, seriously, make, ...   \n",
       "1           0                                   [crazy, bastard]   \n",
       "2           0                  [hoped, post, something, obvious]   \n",
       "3           0                  [find, scene, funny, fact, kills]   \n",
       "4           0  [bill, kill, bill, saying, five, point, palm, ...   \n",
       "\n",
       "                                       porter_stmmer  \n",
       "0  [kill, bill, siren, awesom, serious, make, wee...  \n",
       "1                                   [crazi, bastard]  \n",
       "2                       [hope, post, someth, obviou]  \n",
       "3                   [find, scene, funni, fact, kill]  \n",
       "4  [bill, kill, bill, say, five, point, palm, exp...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging (POS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \n",
    "               \"V\":wordnet.VERB, \n",
    "               \"J\":wordnet.ADJ, \n",
    "               \"R\":wordnet.ADV\n",
    "              }\n",
    "    \n",
    "train_sents = brown.tagged_sents(categories='news')\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "\n",
    "def pos_tag_wordnet(text, pos_tag_type=\"pos_tag\"):\n",
    "    pos_tagged_text = t2.tag(text)\n",
    "    \n",
    "    # map the pos tagging output with wordnet output \n",
    "    pos_tagged_text = [(word, wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys() else (word, wordnet.NOUN) for (word, pos_tag) in pos_tagged_text ]\n",
    "    return pos_tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hoped', 'v'), ('post', 'n'), ('something', 'n'), ('obvious', 'a')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_wordnet(train['tokenized'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(df):\n",
    "    df['combined_postag_wnet'] = df['tokenized'].apply(lambda x: pos_tag_wordnet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagging(train)\n",
    "pos_tagging(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Depression</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>porter_stmmer</th>\n",
       "      <th>combined_postag_wnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>kill bill siren awesome seriously make week be...</td>\n",
       "      <td>0</td>\n",
       "      <td>[kill, bill, siren, awesome, seriously, make, ...</td>\n",
       "      <td>[kill, bill, siren, awesom, serious, make, wee...</td>\n",
       "      <td>[(kill, v), (bill, n), (siren, n), (awesome, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>crazy bastard</td>\n",
       "      <td>0</td>\n",
       "      <td>[crazy, bastard]</td>\n",
       "      <td>[crazi, bastard]</td>\n",
       "      <td>[(crazy, a), (bastard, n)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>hoped post something obvious</td>\n",
       "      <td>0</td>\n",
       "      <td>[hoped, post, something, obvious]</td>\n",
       "      <td>[hope, post, someth, obviou]</td>\n",
       "      <td>[(hoped, v), (post, n), (something, n), (obvio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>find scene funny fact kills</td>\n",
       "      <td>0</td>\n",
       "      <td>[find, scene, funny, fact, kills]</td>\n",
       "      <td>[find, scene, funni, fact, kill]</td>\n",
       "      <td>[(find, v), (scene, n), (funny, n), (fact, n),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>bill kill bill saying five point palm explodin...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bill, kill, bill, saying, five, point, palm, ...</td>\n",
       "      <td>[bill, kill, bill, say, five, point, palm, exp...</td>\n",
       "      <td>[(bill, n), (kill, v), (bill, n), (saying, v),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                               TEXT  \\\n",
       "0  test_subject1005  kill bill siren awesome seriously make week be...   \n",
       "1  test_subject1005                                      crazy bastard   \n",
       "2  test_subject1005                       hoped post something obvious   \n",
       "3  test_subject1005                        find scene funny fact kills   \n",
       "4  test_subject1005  bill kill bill saying five point palm explodin...   \n",
       "\n",
       "   Depression                                          tokenized  \\\n",
       "0           0  [kill, bill, siren, awesome, seriously, make, ...   \n",
       "1           0                                   [crazy, bastard]   \n",
       "2           0                  [hoped, post, something, obvious]   \n",
       "3           0                  [find, scene, funny, fact, kills]   \n",
       "4           0  [bill, kill, bill, saying, five, point, palm, ...   \n",
       "\n",
       "                                       porter_stmmer  \\\n",
       "0  [kill, bill, siren, awesom, serious, make, wee...   \n",
       "1                                   [crazi, bastard]   \n",
       "2                       [hope, post, someth, obviou]   \n",
       "3                   [find, scene, funni, fact, kill]   \n",
       "4  [bill, kill, bill, say, five, point, palm, exp...   \n",
       "\n",
       "                                combined_postag_wnet  \n",
       "0  [(kill, v), (bill, n), (siren, n), (awesome, n...  \n",
       "1                         [(crazy, a), (bastard, n)]  \n",
       "2  [(hoped, v), (post, n), (something, n), (obvio...  \n",
       "3  [(find, v), (scene, n), (funny, n), (fact, n),...  \n",
       "4  [(bill, n), (kill, v), (bill, n), (saying, v),...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without POS Tagging\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(df):\n",
    "    df['lemmatize_word_wo_pos'] = df['tokenized'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    df['lemmatize_word_wo_pos'] = df['lemmatize_word_wo_pos'].apply(lambda x: [word for word in x if word not in stop])\n",
    "    df['lemmatize_text_wo'] = [' '.join(map(str, l)) for l in df['lemmatize_word_wo_pos']] # join back to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization(train)\n",
    "lemmatization(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kill', 'v'), ('bill', 'n'), ('siren', 'n'), ('awesome', 'n'), ('seriously', 'r'), ('make', 'v'), ('week', 'n'), ('better', 'a'), ('call', 'v'), ('saul', 'n'), ('subreddit', 'n'), ('would', 'n'), ('extremely', 'r'), ('grateful', 'a')]\n",
      "['kill', 'bill', 'siren', 'awesome', 'seriously', 'make', 'week', 'better', 'call', 'saul', 'subreddit', 'would', 'extremely', 'grateful']\n"
     ]
    }
   ],
   "source": [
    "print(train[\"combined_postag_wnet\"][0])\n",
    "print(train[\"lemmatize_word_wo_pos\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization with pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with POS Tagging\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization_pos(df):\n",
    "    df['lemmatize_word_w_pos'] = df['combined_postag_wnet'].apply(lambda x: lemmatize_word(x))\n",
    "    df['lemmatize_word_w_pos'] = df['lemmatize_word_w_pos'].apply(lambda x: [word for word in x if word not in stop]) # double check to remove stop words\n",
    "    df['lemmatize_text'] = [' '.join(map(str, l)) for l in df['lemmatize_word_w_pos']] # join back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization_pos(train)\n",
    "lemmatization_pos(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seriously cannot stop singing song arizona moon shining\n",
      "[('seriously', 'r'), ('can', 'n'), ('not', 'n'), ('stop', 'v'), ('singing', 'v'), ('song', 'n'), ('arizona', 'n'), ('moon', 'n'), ('shining', 'n')]\n",
      "['seriously', 'stop', 'singing', 'song', 'arizona', 'moon', 'shining']\n",
      "['seriously', 'stop', 'sing', 'song', 'arizona', 'moon', 'shining']\n",
      "seriously stop sing song arizona moon shining\n"
     ]
    }
   ],
   "source": [
    "print(train[\"TEXT\"][9])\n",
    "print(train[\"combined_postag_wnet\"][9])\n",
    "print(train[\"lemmatize_word_wo_pos\"][9])\n",
    "print(train[\"lemmatize_word_w_pos\"][9])\n",
    "print(train[\"lemmatize_text\"][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kill bill siren awesome seriously make week better call saul subreddit would extremely grateful'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'kill bill siren awesome seriously make week good call saul subreddit would extremely grateful'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'jump conclusions application'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'jump conclusion application'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'know talking much assure'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'know talk much assure'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'tell us die like dogs'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'tell u die like dog'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'hahaha exactly looking lost boys damn good way'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'hahaha exactly look lose boy damn good way'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train[\"TEXT\"][0], train[\"lemmatize_text\"][0])\n",
    "display(train[\"TEXT\"][5], train[\"lemmatize_text\"][5])\n",
    "display(train[\"TEXT\"][30], train[\"lemmatize_text\"][30])\n",
    "display(train[\"TEXT\"][15], train[\"lemmatize_text\"][15])\n",
    "display(train[\"TEXT\"][20], train[\"lemmatize_text\"][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Depression</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>porter_stmmer</th>\n",
       "      <th>combined_postag_wnet</th>\n",
       "      <th>lemmatize_word_wo_pos</th>\n",
       "      <th>lemmatize_text_wo</th>\n",
       "      <th>lemmatize_word_w_pos</th>\n",
       "      <th>lemmatize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>kill bill siren awesome seriously make week be...</td>\n",
       "      <td>0</td>\n",
       "      <td>[kill, bill, siren, awesome, seriously, make, ...</td>\n",
       "      <td>[kill, bill, siren, awesom, serious, make, wee...</td>\n",
       "      <td>[(kill, v), (bill, n), (siren, n), (awesome, n...</td>\n",
       "      <td>[kill, bill, siren, awesome, seriously, make, ...</td>\n",
       "      <td>kill bill siren awesome seriously make week be...</td>\n",
       "      <td>[kill, bill, siren, awesome, seriously, make, ...</td>\n",
       "      <td>kill bill siren awesome seriously make week go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>crazy bastard</td>\n",
       "      <td>0</td>\n",
       "      <td>[crazy, bastard]</td>\n",
       "      <td>[crazi, bastard]</td>\n",
       "      <td>[(crazy, a), (bastard, n)]</td>\n",
       "      <td>[crazy, bastard]</td>\n",
       "      <td>crazy bastard</td>\n",
       "      <td>[crazy, bastard]</td>\n",
       "      <td>crazy bastard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>hoped post something obvious</td>\n",
       "      <td>0</td>\n",
       "      <td>[hoped, post, something, obvious]</td>\n",
       "      <td>[hope, post, someth, obviou]</td>\n",
       "      <td>[(hoped, v), (post, n), (something, n), (obvio...</td>\n",
       "      <td>[hoped, post, something, obvious]</td>\n",
       "      <td>hoped post something obvious</td>\n",
       "      <td>[hop, post, something, obvious]</td>\n",
       "      <td>hop post something obvious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>find scene funny fact kills</td>\n",
       "      <td>0</td>\n",
       "      <td>[find, scene, funny, fact, kills]</td>\n",
       "      <td>[find, scene, funni, fact, kill]</td>\n",
       "      <td>[(find, v), (scene, n), (funny, n), (fact, n),...</td>\n",
       "      <td>[find, scene, funny, fact, kill]</td>\n",
       "      <td>find scene funny fact kill</td>\n",
       "      <td>[find, scene, funny, fact, kill]</td>\n",
       "      <td>find scene funny fact kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_subject1005</td>\n",
       "      <td>bill kill bill saying five point palm explodin...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bill, kill, bill, saying, five, point, palm, ...</td>\n",
       "      <td>[bill, kill, bill, say, five, point, palm, exp...</td>\n",
       "      <td>[(bill, n), (kill, v), (bill, n), (saying, v),...</td>\n",
       "      <td>[bill, kill, bill, saying, five, point, palm, ...</td>\n",
       "      <td>bill kill bill saying five point palm explodin...</td>\n",
       "      <td>[bill, kill, bill, say, five, point, palm, exp...</td>\n",
       "      <td>bill kill bill say five point palm exploding h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                               TEXT  \\\n",
       "0  test_subject1005  kill bill siren awesome seriously make week be...   \n",
       "1  test_subject1005                                      crazy bastard   \n",
       "2  test_subject1005                       hoped post something obvious   \n",
       "3  test_subject1005                        find scene funny fact kills   \n",
       "4  test_subject1005  bill kill bill saying five point palm explodin...   \n",
       "\n",
       "   Depression                                          tokenized  \\\n",
       "0           0  [kill, bill, siren, awesome, seriously, make, ...   \n",
       "1           0                                   [crazy, bastard]   \n",
       "2           0                  [hoped, post, something, obvious]   \n",
       "3           0                  [find, scene, funny, fact, kills]   \n",
       "4           0  [bill, kill, bill, saying, five, point, palm, ...   \n",
       "\n",
       "                                       porter_stmmer  \\\n",
       "0  [kill, bill, siren, awesom, serious, make, wee...   \n",
       "1                                   [crazi, bastard]   \n",
       "2                       [hope, post, someth, obviou]   \n",
       "3                   [find, scene, funni, fact, kill]   \n",
       "4  [bill, kill, bill, say, five, point, palm, exp...   \n",
       "\n",
       "                                combined_postag_wnet  \\\n",
       "0  [(kill, v), (bill, n), (siren, n), (awesome, n...   \n",
       "1                         [(crazy, a), (bastard, n)]   \n",
       "2  [(hoped, v), (post, n), (something, n), (obvio...   \n",
       "3  [(find, v), (scene, n), (funny, n), (fact, n),...   \n",
       "4  [(bill, n), (kill, v), (bill, n), (saying, v),...   \n",
       "\n",
       "                               lemmatize_word_wo_pos  \\\n",
       "0  [kill, bill, siren, awesome, seriously, make, ...   \n",
       "1                                   [crazy, bastard]   \n",
       "2                  [hoped, post, something, obvious]   \n",
       "3                   [find, scene, funny, fact, kill]   \n",
       "4  [bill, kill, bill, saying, five, point, palm, ...   \n",
       "\n",
       "                                   lemmatize_text_wo  \\\n",
       "0  kill bill siren awesome seriously make week be...   \n",
       "1                                      crazy bastard   \n",
       "2                       hoped post something obvious   \n",
       "3                         find scene funny fact kill   \n",
       "4  bill kill bill saying five point palm explodin...   \n",
       "\n",
       "                                lemmatize_word_w_pos  \\\n",
       "0  [kill, bill, siren, awesome, seriously, make, ...   \n",
       "1                                   [crazy, bastard]   \n",
       "2                    [hop, post, something, obvious]   \n",
       "3                   [find, scene, funny, fact, kill]   \n",
       "4  [bill, kill, bill, say, five, point, palm, exp...   \n",
       "\n",
       "                                      lemmatize_text  \n",
       "0  kill bill siren awesome seriously make week go...  \n",
       "1                                      crazy bastard  \n",
       "2                         hop post something obvious  \n",
       "3                         find scene funny fact kill  \n",
       "4  bill kill bill say five point palm exploding h...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 531394 entries, 0 to 531393\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count   Dtype \n",
      "---  ------                 --------------   ----- \n",
      " 0   ID                     531394 non-null  object\n",
      " 1   TEXT                   531394 non-null  object\n",
      " 2   Depression             531394 non-null  int64 \n",
      " 3   tokenized              531394 non-null  object\n",
      " 4   porter_stmmer          531394 non-null  object\n",
      " 5   combined_postag_wnet   531394 non-null  object\n",
      " 6   lemmatize_word_wo_pos  531394 non-null  object\n",
      " 7   lemmatize_text_wo      531394 non-null  object\n",
      " 8   lemmatize_word_w_pos   531394 non-null  object\n",
      " 9   lemmatize_text         531394 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 40.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Depression</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>porter_stmmer</th>\n",
       "      <th>combined_postag_wnet</th>\n",
       "      <th>lemmatize_word_wo_pos</th>\n",
       "      <th>lemmatize_text_wo</th>\n",
       "      <th>lemmatize_word_w_pos</th>\n",
       "      <th>lemmatize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>test_subject1085</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>test_subject1085</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>test_subject1103</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>test_subject1103</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>test_subject1103</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530660</th>\n",
       "      <td>train_subject9974</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531126</th>\n",
       "      <td>train_subject9974</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531246</th>\n",
       "      <td>train_subject9974</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531281</th>\n",
       "      <td>train_subject9974</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531382</th>\n",
       "      <td>train_subject9974</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5598 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID TEXT  Depression tokenized porter_stmmer  \\\n",
       "465      test_subject1085                0        []            []   \n",
       "466      test_subject1085                0        []            []   \n",
       "500      test_subject1103                0        []            []   \n",
       "561      test_subject1103                0        []            []   \n",
       "572      test_subject1103                0        []            []   \n",
       "...                   ...  ...         ...       ...           ...   \n",
       "530660  train_subject9974                0        []            []   \n",
       "531126  train_subject9974                0        []            []   \n",
       "531246  train_subject9974                0        []            []   \n",
       "531281  train_subject9974                0        []            []   \n",
       "531382  train_subject9974                0        []            []   \n",
       "\n",
       "       combined_postag_wnet lemmatize_word_wo_pos lemmatize_text_wo  \\\n",
       "465                      []                    []                     \n",
       "466                      []                    []                     \n",
       "500                      []                    []                     \n",
       "561                      []                    []                     \n",
       "572                      []                    []                     \n",
       "...                     ...                   ...               ...   \n",
       "530660                   []                    []                     \n",
       "531126                   []                    []                     \n",
       "531246                   []                    []                     \n",
       "531281                   []                    []                     \n",
       "531382                   []                    []                     \n",
       "\n",
       "       lemmatize_word_w_pos lemmatize_text  \n",
       "465                      []                 \n",
       "466                      []                 \n",
       "500                      []                 \n",
       "561                      []                 \n",
       "572                      []                 \n",
       "...                     ...            ...  \n",
       "530660                   []                 \n",
       "531126                   []                 \n",
       "531246                   []                 \n",
       "531281                   []                 \n",
       "531382                   []                 \n",
       "\n",
       "[5598 rows x 10 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['TEXT'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[~((train['TEXT'] == '') |(train['lemmatize_text'] == '') | (train['lemmatize_text_wo'] == '') )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[~((test['TEXT'] == '') |(test['lemmatize_text'] == '') | (test['lemmatize_text_wo'] == '') )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 525700 entries, 0 to 531393\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count   Dtype \n",
      "---  ------                 --------------   ----- \n",
      " 0   ID                     525700 non-null  object\n",
      " 1   TEXT                   525700 non-null  object\n",
      " 2   Depression             525700 non-null  int64 \n",
      " 3   tokenized              525700 non-null  object\n",
      " 4   porter_stmmer          525700 non-null  object\n",
      " 5   combined_postag_wnet   525700 non-null  object\n",
      " 6   lemmatize_word_wo_pos  525700 non-null  object\n",
      " 7   lemmatize_text_wo      525700 non-null  object\n",
      " 8   lemmatize_word_w_pos   525700 non-null  object\n",
      " 9   lemmatize_text         525700 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 44.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 539120 entries, 0 to 544446\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count   Dtype \n",
      "---  ------                 --------------   ----- \n",
      " 0   ID                     539120 non-null  object\n",
      " 1   TEXT                   539120 non-null  object\n",
      " 2   Depression             539120 non-null  int64 \n",
      " 3   tokenized              539120 non-null  object\n",
      " 4   porter_stmmer          539120 non-null  object\n",
      " 5   combined_postag_wnet   539120 non-null  object\n",
      " 6   lemmatize_word_wo_pos  539120 non-null  object\n",
      " 7   lemmatize_text_wo      539120 non-null  object\n",
      " 8   lemmatize_word_w_pos   539120 non-null  object\n",
      " 9   lemmatize_text         539120 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 45.2+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['ID','TEXT','lemmatize_text','lemmatize_text_wo','Depression']]\n",
    "test = test[['ID','TEXT','lemmatize_text','lemmatize_text_wo','Depression']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dft = train.groupby('ID',group_keys=True)['TEXT'].agg(lambda x: ','.join(x.tolist()))\n",
    "dfd = train.groupby('ID')['Depression'].unique()\n",
    "dfi = train.groupby('ID')['ID'].unique()'''\n",
    "df_train = pd.concat ([train.groupby('ID')['ID'].unique() ,  train.groupby('ID',group_keys=True)['lemmatize_text'].agg(lambda x: ','.join(x.tolist())) , train.groupby('ID')['Depression'].unique().astype('int')],axis = 1)\n",
    "\n",
    "\n",
    "df_test = pd.concat ([test.groupby('ID')['ID'].unique() ,  test.groupby('ID',group_keys=True)['lemmatize_text'].agg(lambda x: ','.join(x.tolist())) , test.groupby('ID')['Depression'].unique().astype('int')],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>lemmatize_text</th>\n",
       "      <th>Depression</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test_subject1005</th>\n",
       "      <td>[test_subject1005]</td>\n",
       "      <td>kill bill siren awesome seriously make week go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject1062</th>\n",
       "      <td>[test_subject1062]</td>\n",
       "      <td>wind chill become heat index know wind chill u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject1085</th>\n",
       "      <td>[test_subject1085]</td>\n",
       "      <td>exactly know get job bookstore without least k...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject1103</th>\n",
       "      <td>[test_subject1103]</td>\n",
       "      <td>tell 16 year old would get angry day old infan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject1111</th>\n",
       "      <td>[test_subject1111]</td>\n",
       "      <td>really see greek want rude canadian though,buy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ID  \\\n",
       "ID                                     \n",
       "test_subject1005  [test_subject1005]   \n",
       "test_subject1062  [test_subject1062]   \n",
       "test_subject1085  [test_subject1085]   \n",
       "test_subject1103  [test_subject1103]   \n",
       "test_subject1111  [test_subject1111]   \n",
       "\n",
       "                                                     lemmatize_text  \\\n",
       "ID                                                                    \n",
       "test_subject1005  kill bill siren awesome seriously make week go...   \n",
       "test_subject1062  wind chill become heat index know wind chill u...   \n",
       "test_subject1085  exactly know get job bookstore without least k...   \n",
       "test_subject1103  tell 16 year old would get angry day old infan...   \n",
       "test_subject1111  really see greek want rude canadian though,buy...   \n",
       "\n",
       "                  Depression  \n",
       "ID                            \n",
       "test_subject1005           0  \n",
       "test_subject1062           0  \n",
       "test_subject1085           0  \n",
       "test_subject1103           0  \n",
       "test_subject1111           0  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>887.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.152198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.359416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Depression\n",
       "count  887.000000\n",
       "mean     0.152198\n",
       "std      0.359416\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      0.000000\n",
       "75%      0.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 887 entries, test_subject1005 to train_subject9974\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   ID              887 non-null    object\n",
      " 1   lemmatize_text  887 non-null    object\n",
      " 2   Depression      887 non-null    int32 \n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 24.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 820 entries, subject1010 to subject998\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   ID              820 non-null    object\n",
      " 1   lemmatize_text  820 non-null    object\n",
      " 2   Depression      820 non-null    int32 \n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 22.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  export csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train_clean.csv', index=None)\n",
    "df_test.to_csv('test_clean.csv', index=None)\n",
    "\n",
    "df_train_wo.to_csv('train_clean_wo.csv', index=None)\n",
    "df_test_wo.to_csv('test_clean_wo.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
