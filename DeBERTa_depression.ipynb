{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9812\\491656687.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  from tqdm._tqdm_notebook import tqdm_notebook\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback, set_seed\n",
    "\n",
    "# Progress bar\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    741\n",
       "1     79\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train_clean.csv', encoding='utf8')   \n",
    "test = pd.read_csv('test_clean.csv', encoding='utf8')   \n",
    "\n",
    "train.rename(columns = {'lemmatize_text':'text'}, inplace = True)\n",
    "train.rename(columns = {'Depression':'label'}, inplace = True)\n",
    "test.rename(columns = {'lemmatize_text':'text'}, inplace = True)\n",
    "test.rename(columns = {'Depression':'label'}, inplace = True)\n",
    "\n",
    "train = train[~train['text'].isna()]\n",
    "train = train[train['text'] != \"\"]\n",
    "\n",
    "test = test[~test['text'].isna()]\n",
    "test = test[test['text'] != \"\"]\n",
    "\n",
    "train.drop(['ID'],inplace= True,axis=1)\n",
    "test.drop(['ID'],inplace= True,axis=1)\n",
    "train['label'].value_counts()\n",
    "test['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# Define pretrained tokenizer and model\n",
    "\n",
    "model_name = \"microsoft/deberta-large-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# def model_init():\n",
    "#     return AutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "#                                                               num_labels=2, \n",
    "#                                                             output_attentions = False, # Whether the model returns attentions weights.\n",
    "#                                                             output_hidden_states = False,\n",
    "#                                                             return_dict=True )\n",
    "set_seed(1457)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Data: Text Input and Label #\n",
    "##############################\n",
    "\n",
    "\n",
    "########################\n",
    "# Create torch dataset #\n",
    "########################\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "\n",
    "#############################\n",
    "# Define Trainer parameters #\n",
    "#############################\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall_cb = recall_score(y_true=labels, y_pred=pred, average='binary', pos_label=1)\n",
    "    precision_cb = precision_score(y_true=labels, y_pred=pred, average='binary', pos_label=1)\n",
    "    f1_cb = f1_score(y_true=labels, y_pred=pred, average='binary', pos_label=1)\n",
    "    \n",
    "    recall_ncb = recall_score(y_true=labels, y_pred=pred, average='binary', pos_label=0)\n",
    "    precision_ncb = precision_score(y_true=labels, y_pred=pred, average='binary', pos_label=0)\n",
    "    f1_ncb = f1_score(y_true=labels, y_pred=pred, average='binary', pos_label=0)\n",
    "    \n",
    "    recall_overall = recall_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    precision_overall = precision_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    f1_overall = f1_score(y_true=labels, y_pred=pred, average='macro')\n",
    "\n",
    "\n",
    "    return {\"accuracy\": accuracy, \n",
    "            \"precision_cb\": precision_cb, \"recall_cb\": recall_cb, \"f1_cb\": f1_cb,\n",
    "            \"precision_ncb\": precision_ncb, \"recall_ncb\": recall_ncb, \"f1_ncb\": f1_ncb,\n",
    "            \"precision_overall\": precision_overall, \"recall_overall\": recall_overall, \"f1_overall\": f1_overall}\n",
    "\n",
    "\n",
    "#########################\n",
    "# Plot Confusion Matrix #\n",
    "#########################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "\n",
    "    # CONFUSION MATRIX IN PERCENTAGE\n",
    "    cf_pct = cf.astype('float')/cf.sum(axis=1)[:, np.newaxis]\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf_pct.flatten()]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf_pct,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "# Run Hold Out Test #\n",
    "#####################\n",
    "\n",
    "# Train and Test Set\n",
    "X_train, y_train, X_test,y_test = list(train['text']), list(train['label']), list(test['text']), list(test['label'])\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=1127)\n",
    "\n",
    "# Train and Validate Set\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Create torch dataset\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_test)\n",
    "\n",
    "def run_hold_out_split(model_name='DistilBert',\n",
    "                       epoch=8,\n",
    "                       train_dataset=train_dataset,\n",
    "                       eval_dataset=val_dataset,\n",
    "                       checkpoint=False):\n",
    "  \n",
    "    print(\"Developing Model with Hold Out Splits for: \" + model_name)\n",
    "    # Fine Tune Transformer\n",
    "    # Define Trainer\n",
    "    args = TrainingArguments(\n",
    "      output_dir=\"content/drive/MyDrive_binary/output_\" + model_name + \"/holdout\",\n",
    "      evaluation_strategy=\"epoch\",\n",
    "      save_strategy=\"epoch\",\n",
    "      #eval_steps=500,\n",
    "      #per_device_train_batch_size=1,\n",
    "      #per_device_eval_batch_size=1,\n",
    "      num_train_epochs=epoch, #1 was okay\n",
    "      seed=1127,\n",
    "      load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "    #model_init=model_init,\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    "    )\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    print(\"Complete for hold-out validate set\")\n",
    "\n",
    "\n",
    "    \n",
    "###########################\n",
    "# Predict (Hold Out Test) #\n",
    "###########################\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def compute_metrics_holdout(model_name='DeBERTa',\n",
    "                            model_path='content/drive/MyDrive_binary/output_DeBERTa/holdout/checkpoint-3820', \n",
    "                            average_method='binary',\n",
    "                            X_test=X_test):\n",
    "  \n",
    "    X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Create torch dataset\n",
    "    test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "    # Load trained model\n",
    "    model_pred = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "    # Define test trainer\n",
    "    test_trainer = Trainer(model_pred)\n",
    "\n",
    "    # Make prediction\n",
    "    raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "\n",
    "    # Preprocess raw predictions\n",
    "    y_pred = np.argmax(raw_pred, axis=1)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision_cb = precision_score(y_test, y_pred, average=average_method, pos_label=1)\n",
    "    recall_cb = recall_score(y_test, y_pred, average=average_method, pos_label=1)\n",
    "    f1_cb = f1_score(y_test, y_pred, average=average_method, pos_label=1)\n",
    "\n",
    "    precision_ncb = precision_score(y_test, y_pred, average=average_method, pos_label=0)\n",
    "    recall_ncb = recall_score(y_test, y_pred, average=average_method, pos_label=0)\n",
    "    f1_ncb = f1_score(y_test, y_pred, average=average_method, pos_label=0)\n",
    "\n",
    "    precision_overall = precision_score(y_test, y_pred, average='macro')\n",
    "    recall_overall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1_overall = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Print Results\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print()\n",
    "    print(\"Label 1: Depression\")\n",
    "    print(\"Precision: \", precision_cb)\n",
    "    print(\"Recall: \", recall_cb)\n",
    "    print(\"F-measure: \", f1_cb)\n",
    "    print()\n",
    "    print(\"Label 0: Non-Depression\")\n",
    "    print(\"Precision: \", precision_ncb)\n",
    "    print(\"Recall: \", recall_ncb)\n",
    "    print(\"F-measure: \", f1_ncb)\n",
    "    print()\n",
    "    print(\"Macro Metrics\")\n",
    "    print(\"Precision: \", precision_overall)\n",
    "    print(\"Recall: \", recall_overall)\n",
    "    print(\"F-measure: \", f1_overall)\n",
    "    print()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_mat = confusion_matrix(y_test,y_pred)\n",
    "    categories = ['Non-Depression', 'Depression']\n",
    "    labels = ['True Negative','',\n",
    "            '','True Positive']\n",
    "\n",
    "    make_confusion_matrix(conf_mat, \n",
    "                        group_names=labels,\n",
    "                        categories=categories, \n",
    "                        figsize=(8,5), \n",
    "                        cbar=True, \n",
    "                        title='Fine Tuned ' + model_name + ' for Depression Detection', \n",
    "                        cmap='YlGnBu', \n",
    "                        sum_stats=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#holdout\n",
    "run_hold_out_split(model_name='DeBERTa',\n",
    "                       epoch=8,\n",
    "                       train_dataset=train_dataset,\n",
    "                       eval_dataset=val_dataset,\n",
    "                       checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#holdout\n",
    "run_hold_out_split(model_name='DeBERTa',\n",
    "                       epoch=8,\n",
    "                       train_dataset=train_dataset,\n",
    "                       eval_dataset=val_dataset,\n",
    "                       checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics_holdout(model_name='roberta-base',\n",
    "                        model_path='content/drive/MyDrive_binary/DeBERTa/holdout/checkpoint-111', \n",
    "                        average_method='binary',\n",
    "                        X_test=X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
